# Multimodal-RAG-Fashion-Assistant
**Notebook For Demonstrating Multimodal RAG For A Fashion Assistant**
This notebook demonstrates the use of a multimodal retrieval-augmented generation (RAG) framework to assist in shopping for example, a gray jacket.
OpenCLIP, ChromaDB, LangChain, and GPT-4 with vision capabilities are integrated into the notebook. 
Technologies Used:
**Fashionpedia Dataset**: A rich dataset available through HuggingFace with annotations for a large set of fashion items which will serve as our primary data source. 
**OpenCLIP**: Leveraged for generating multimodal embeddings that help in understanding and linking visual content with textual descriptions. 
**ChromaDB**: A vector database used to store and retrieve the multimodal embeddings efficiently. 
**LangChain**: Used for orchestrating the flow of data and queries between different models and databases.
**GPT4o**: Vision capable LLM from OpenAI.
